# vLLM instance (`VLLM_API_BASE` is also valid)
HOSTED_VLLM_API_BASE=https://helx-ai-medium-vllm.apps.renci.org/v1
# vLLM API Key (if applicable; `VLLM_API_KEY` is also valid)
HOSTED_VLLM_API_KEY=<key>
# Number of attempts to load the vLLM's available models endpoint before giving up.
HOSTED_VLLM_LOAD_AVAILABLE_MODELS_RETRIES=5
# Backoff factor between attempts to load vLLM available model endpoint.
# Each backoff is calculated by FACTOR * 2^NUM_PREV_RETRIES
HOSTED_VLLM_LOAD_AVAILABLE_MODELS_BACKOFF=0.25

# Bdf-pz package-level logging level
LOG_LEVEL=info
# Shutdown beaker if vLLM is configured but available models are unable to be loaded.
DEBUG=true