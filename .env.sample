# vLLM instance (`VLLM_API_BASE` is also valid)
HOSTED_VLLM_API_BASE=https://helx-ai-medium-vllm.apps.renci.org/v1
# vLLM API Key (if applicable; `VLLM_API_KEY` is also valid)
HOSTED_VLLM_API_KEY=<key>

# Endpoint for Azure OpenAI proxy (direct usage of Azure is not yet supported.)
AZURE_OPENAI_ENDPOINT="https://vllm-fastapi.apps.renci.org/"
# (`AZURE_OPENAI_API_KEY` is also valid)
AZURE_OPENAI_KEY="<key>"
# Associated Azure model deployment
AZURE_OPENAI_DEPLOYMENT="<deployment>"


# Bdf-pz package-level logging level
LOG_LEVEL=info
# Shutdown beaker if vLLM is configured but available models are unable to be loaded.
DEBUG=true